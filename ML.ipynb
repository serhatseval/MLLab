{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating Spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import soundfile as sf\n",
    "import pathlib as pl\n",
    "import librosa\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import noisereduce as nr\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "def plot_spectrogram(signal, output_path, interval_index, f, file_name, class_val):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=44100, n_mels=128, fmax=8000)\n",
    "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Save the Mel spectrogram as an image without title and borders\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.axis('off')  # Remove axes\n",
    "    plt.margins(0, 0)  # Remove margins\n",
    "    plt.gca().set_axis_off()  # Remove axis lines\n",
    "    plt.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)  # Remove padding\n",
    "    plt.gcf().set_size_inches(10, 4)  # Set figure size\n",
    "    librosa.display.specshow(mel_spectrogram_db, sr=44100, x_axis='time', y_axis='mel', fmax=8000, cmap='gray_r')\n",
    "    plt.savefig(f\"{output_path / f'{file_name}_i{interval_index}.png'}\", bbox_inches='tight', pad_inches=0, format='png')\n",
    "    plt.close()\n",
    "\n",
    "    # Write the file path to the output file\n",
    "    f.write(f\"{output_path / f'{file_name}_i{interval_index}.png'},f'{class_val}\\n\")  # Change it to 1 when it's creating allowed class\n",
    "\n",
    "    return mel_spectrogram_db\n",
    "\n",
    "\n",
    "def plot_spectrogram_main(file_path, file_name, class_val):\n",
    "\n",
    "    joined_path = str(os.path.join(file_path, file_name))\n",
    "    print(f\"Original audio duration: {librosa.get_duration(path=joined_path)} seconds\")\n",
    "    signal, signal_rate = sf.read(joined_path)\n",
    "    f = open(\"MEL_Spectograms/labels.csv\", \"a\")\n",
    "    # Trim silence\n",
    "    signal, _ = librosa.effects.trim(signal, top_db=20)\n",
    "    print(f\"Audio duration after trimming silence: {len(signal) / signal_rate:.2f} seconds\")\n",
    "    non_silent_intervals = librosa.effects.split(signal, top_db=20)\n",
    "    processed_segments = []\n",
    "    for start, end in non_silent_intervals:\n",
    "        segment = signal[start:end]\n",
    "        processed_segments.append(segment)\n",
    "    signal = np.concatenate(processed_segments)  # Combine segments\n",
    "\n",
    "    reduced_noise = nr.reduce_noise(signal, sr=signal_rate)\n",
    "    signal = reduced_noise\n",
    "    print(f\"Audio duration after reducing noise: {len(signal) / signal_rate:.2f} seconds\")\n",
    "\n",
    "    #Split into 3-second intervals\n",
    "    interval_duration = 3 * signal_rate\n",
    "    num_intervals = int(np.floor(len(signal) / interval_duration))\n",
    "\n",
    "    for i in range(num_intervals):\n",
    "        start = i * interval_duration\n",
    "        end = min((i + 1) * interval_duration, len(signal))\n",
    "        interval_signal = signal[start:end]\n",
    "\n",
    "        print(\n",
    "            f\"Processing interval {i + 1}/{num_intervals}, duration: {len(interval_signal) / signal_rate:.2f} seconds\")\n",
    "        spectrogram_db = plot_spectrogram(\n",
    "            interval_signal, pl.Path('MEL_Spectograms/images'), i + 1, f, file_name, class_val)\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating spectrograms for accepting class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# assign directory\n",
    "directory = \"AudioFiles\"\n",
    "x = 0\n",
    "\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in files:\n",
    "        if (re.match(r'^(f1_|f7|f8|m3|m6|m8)', filename)) and (re.match(r'.*\\.wav$', filename)) and not filename.startswith((\".\", \"_\")):\n",
    "            plot_spectrogram_main(root, filename, 1)\n",
    "            x += 1\n",
    "            print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating spectrograms for rejecting class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required module\n",
    "import os\n",
    "import re\n",
    "from Plot_Spectrograms import main\n",
    "\n",
    "# assign directory\n",
    "directory = \"AudioFiles\"\n",
    "x = 0\n",
    "\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in files:\n",
    "        if (not re.match(r'^(f1_|f7|f8|m3|m6|m8)', filename)) and (re.match(r'.*\\.wav$', filename)) and not filename.startswith((\".\", \"_\")):\n",
    "            main(root, filename, 0)\n",
    "            x += 1\n",
    "            print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Model we will use the labels that we removed labels with 95% similarity with Pruning. We have prepared those labels and it is provided with the spectrograms. Training is done following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "\n",
    "# Paths\n",
    "annotations_file = 'MEL_Spectograms/labels_removing_95_percent_similarity.csv'\n",
    "img_dir = 'MEL_Spectograms/images'\n",
    "\n",
    "# Constants\n",
    "img_height = 1000\n",
    "img_width = 400  # Length for 3 seconds of audio\n",
    "features = 2\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path, mode=ImageReadMode.GRAY)\n",
    "        label = int(self.img_labels.iloc[idx, 1])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "# Device configuration\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# Model creation\n",
    "def create_model():\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "    # Add dropout layer before the final fully connected layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(p=0.5),  # 50% dropout rate\n",
    "        nn.Linear(num_features, features)\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "# Prune the model\n",
    "def prune_model(model, amount=0.5):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')\n",
    "    return model\n",
    "\n",
    "# Training loop\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# Validation loop\n",
    "def validate(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return val_loss, correct, f1, cm\n",
    "\n",
    "# Save results\n",
    "def save_results(filename, accuracy, avg_loss, f1_score, cm):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f\"Accuracy: {accuracy:.4f}, Avg Loss: {avg_loss:.4f}, F1 Score: {f1_score:.4f}\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(f\"{cm}\\n\")\n",
    "\n",
    "# Calculate class weights\n",
    "def calculate_class_weights(dataset):\n",
    "    labels = [dataset[i][1] for i in range(len(dataset))]\n",
    "    class_sample_counts = pd.Series(labels).value_counts().sort_index()\n",
    "    weights = 1.0 / class_sample_counts\n",
    "    class_weights = torch.tensor(weights / weights.sum(), dtype=torch.float32).to(device)\n",
    "    return class_weights\n",
    "\n",
    "# Group intervals by audio file\n",
    "def get_audio_file_groups(annotations_file):\n",
    "    df = pd.read_csv(annotations_file)\n",
    "    groups = defaultdict(list)\n",
    "    for idx, row in df.iterrows():\n",
    "        audio_file = row[0].split('_i')[0]  # Extract the audio file part before '_i'\n",
    "        groups[audio_file].append(idx)  # Map audio file to its indices\n",
    "    return groups\n",
    "\n",
    "# Split audio files into train and test sets\n",
    "def split_by_audio_file(groups, test_size=0.2, random_state=42):\n",
    "    audio_files = list(groups.keys())\n",
    "    train_files, test_files = train_test_split(audio_files, test_size=test_size, random_state=random_state)\n",
    "    train_indices = [idx for file in train_files for idx in groups[file]]\n",
    "    test_indices = [idx for file in test_files for idx in groups[file]]\n",
    "    return train_indices, test_indices\n",
    "\n",
    "# Main logic\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Defining Dataset\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    dataset = CustomImageDataset(annotations_file, img_dir, transform=transform)\n",
    "    groups = get_audio_file_groups(annotations_file)\n",
    "    train_idx, test_idx = split_by_audio_file(groups, test_size=0.2)\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_idx)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    class_weights = calculate_class_weights(train_dataset)\n",
    "    print(f\"Class Weights: {class_weights}\")\n",
    "\n",
    "    model = create_model()\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        val_loss, accuracy, f1_score, cm = validate(test_dataloader, model, loss_fn)\n",
    "        scheduler.step()\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        torch.save(model.state_dict(), f\"Models/FinalPruning/model_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "    train_loss, train_accuracy, train_f1, train_cm = validate(train_dataloader, model, loss_fn)\n",
    "    test_loss, test_accuracy, test_f1, test_cm = validate(test_dataloader, model, loss_fn)\n",
    "\n",
    "    save_results(\"Train_Results_under95_final.txt\", train_accuracy, train_loss, train_f1, train_cm)\n",
    "    save_results(\"Test_Results_under95_final.txt\", test_accuracy, test_loss, test_f1, test_cm)\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "    # Prune the model\n",
    "    pruned_model = prune_model(model, amount=0.5)\n",
    "\n",
    "    # Retrain the pruned model\n",
    "    optimizer = torch.optim.Adam(pruned_model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Retraining Epoch {epoch + 1}\\n-------------------------------\")\n",
    "        train(train_dataloader, pruned_model, loss_fn, optimizer)\n",
    "        val_loss, accuracy, f1_score, cm = validate(test_dataloader, pruned_model, loss_fn)\n",
    "        scheduler.step()\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "        # Save the pruned model after each epoch\n",
    "        torch.save(pruned_model.state_dict(), f\"Models/Pruned/model_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "    train_loss, train_accuracy, train_f1, train_cm = validate(train_dataloader, pruned_model, loss_fn)\n",
    "    test_loss, test_accuracy, test_f1, test_cm = validate(test_dataloader, pruned_model, loss_fn)\n",
    "\n",
    "    save_results(\"Train_Results_pruned_final.txt\", train_accuracy, train_loss, train_f1, train_cm)\n",
    "    save_results(\"Test_Results_pruned_final.txt\", test_accuracy, test_loss, test_f1, test_cm)\n",
    "\n",
    "    print(\"Pruning and retraining done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
