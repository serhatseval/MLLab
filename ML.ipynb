{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Creating Spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from matplotlib import pyplot as plt\n",
    "import soundfile as sf\n",
    "import pathlib as pl\n",
    "import librosa\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import noisereduce as nr\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "\n",
    "def plot_spectrogram(signal, output_path, interval_index, f, file_name, class_val):\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=signal, sr=44100, n_mels=128, fmax=8000)\n",
    "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "    # Save the Mel spectrogram as an image without title and borders\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.axis('off')  # Remove axes\n",
    "    plt.margins(0, 0)  # Remove margins\n",
    "    plt.gca().set_axis_off()  # Remove axis lines\n",
    "    plt.subplots_adjust(top=1, bottom=0, right=1, left=0, hspace=0, wspace=0)  # Remove padding\n",
    "    plt.gcf().set_size_inches(10, 4)  # Set figure size\n",
    "    librosa.display.specshow(mel_spectrogram_db, sr=44100, x_axis='time', y_axis='mel', fmax=8000, cmap='gray_r')\n",
    "    plt.savefig(f\"{output_path / f'{file_name}_i{interval_index}.png'}\", bbox_inches='tight', pad_inches=0, format='png')\n",
    "    plt.close()\n",
    "\n",
    "    # Write the file path to the output file\n",
    "    f.write(f\"{output_path / f'{file_name}_i{interval_index}.png'},f'{class_val}\\n\")  # Change it to 1 when it's creating allowed class\n",
    "\n",
    "    return mel_spectrogram_db\n",
    "\n",
    "\n",
    "def plot_spectrogram_main(file_path, file_name, class_val):\n",
    "\n",
    "    joined_path = str(os.path.join(file_path, file_name))\n",
    "    print(f\"Original audio duration: {librosa.get_duration(path=joined_path)} seconds\")\n",
    "    signal, signal_rate = sf.read(joined_path)\n",
    "    f = open(\"MEL_Spectograms/labels.csv\", \"a\")\n",
    "    # Trim silence\n",
    "    signal, _ = librosa.effects.trim(signal, top_db=20)\n",
    "    print(f\"Audio duration after trimming silence: {len(signal) / signal_rate:.2f} seconds\")\n",
    "    non_silent_intervals = librosa.effects.split(signal, top_db=20)\n",
    "    processed_segments = []\n",
    "    for start, end in non_silent_intervals:\n",
    "        segment = signal[start:end]\n",
    "        processed_segments.append(segment)\n",
    "    signal = np.concatenate(processed_segments)  # Combine segments\n",
    "\n",
    "    reduced_noise = nr.reduce_noise(signal, sr=signal_rate)\n",
    "    signal = reduced_noise\n",
    "    print(f\"Audio duration after reducing noise: {len(signal) / signal_rate:.2f} seconds\")\n",
    "\n",
    "    #Split into 3-second intervals\n",
    "    interval_duration = 3 * signal_rate\n",
    "    num_intervals = int(np.floor(len(signal) / interval_duration))\n",
    "\n",
    "    for i in range(num_intervals):\n",
    "        start = i * interval_duration\n",
    "        end = min((i + 1) * interval_duration, len(signal))\n",
    "        interval_signal = signal[start:end]\n",
    "\n",
    "        print(\n",
    "            f\"Processing interval {i + 1}/{num_intervals}, duration: {len(interval_signal) / signal_rate:.2f} seconds\")\n",
    "        spectrogram_db = plot_spectrogram(\n",
    "            interval_signal, pl.Path('MEL_Spectograms/images'), i + 1, f, file_name, class_val)\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating spectrograms for accepting class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original audio duration: 159.33251700680273 seconds\n",
      "Audio duration after trimming silence: 151.73 seconds\n",
      "Audio duration after reducing noise: 79.11 seconds\n",
      "Processing interval 1/26, duration: 3.00 seconds\n",
      "Processing interval 2/26, duration: 3.00 seconds\n",
      "Processing interval 3/26, duration: 3.00 seconds\n",
      "Processing interval 4/26, duration: 3.00 seconds\n",
      "Processing interval 5/26, duration: 3.00 seconds\n",
      "Processing interval 6/26, duration: 3.00 seconds\n",
      "Processing interval 7/26, duration: 3.00 seconds\n",
      "Processing interval 8/26, duration: 3.00 seconds\n",
      "Processing interval 9/26, duration: 3.00 seconds\n",
      "Processing interval 10/26, duration: 3.00 seconds\n",
      "Processing interval 11/26, duration: 3.00 seconds\n",
      "Processing interval 12/26, duration: 3.00 seconds\n",
      "Processing interval 13/26, duration: 3.00 seconds\n",
      "Processing interval 14/26, duration: 3.00 seconds\n",
      "Processing interval 15/26, duration: 3.00 seconds\n",
      "Processing interval 16/26, duration: 3.00 seconds\n",
      "Processing interval 17/26, duration: 3.00 seconds\n",
      "Processing interval 18/26, duration: 3.00 seconds\n",
      "Processing interval 19/26, duration: 3.00 seconds\n",
      "Processing interval 20/26, duration: 3.00 seconds\n",
      "Processing interval 21/26, duration: 3.00 seconds\n",
      "Processing interval 22/26, duration: 3.00 seconds\n",
      "Processing interval 23/26, duration: 3.00 seconds\n",
      "Processing interval 24/26, duration: 3.00 seconds\n",
      "Processing interval 25/26, duration: 3.00 seconds\n",
      "Processing interval 26/26, duration: 3.00 seconds\n",
      "f8_script5_ipad_livingroom1.wav\n",
      "Original audio duration: 178.77832199546486 seconds\n",
      "Audio duration after trimming silence: 170.24 seconds\n",
      "Audio duration after reducing noise: 89.93 seconds\n",
      "Processing interval 1/29, duration: 3.00 seconds\n",
      "Processing interval 2/29, duration: 3.00 seconds\n",
      "Processing interval 3/29, duration: 3.00 seconds\n",
      "Processing interval 4/29, duration: 3.00 seconds\n",
      "Processing interval 5/29, duration: 3.00 seconds\n",
      "Processing interval 6/29, duration: 3.00 seconds\n",
      "Processing interval 7/29, duration: 3.00 seconds\n",
      "Processing interval 8/29, duration: 3.00 seconds\n",
      "Processing interval 9/29, duration: 3.00 seconds\n",
      "Processing interval 10/29, duration: 3.00 seconds\n",
      "Processing interval 11/29, duration: 3.00 seconds\n",
      "Processing interval 12/29, duration: 3.00 seconds\n",
      "Processing interval 13/29, duration: 3.00 seconds\n",
      "Processing interval 14/29, duration: 3.00 seconds\n",
      "Processing interval 15/29, duration: 3.00 seconds\n",
      "Processing interval 16/29, duration: 3.00 seconds\n",
      "Processing interval 17/29, duration: 3.00 seconds\n",
      "Processing interval 18/29, duration: 3.00 seconds\n",
      "Processing interval 19/29, duration: 3.00 seconds\n",
      "Processing interval 20/29, duration: 3.00 seconds\n",
      "Processing interval 21/29, duration: 3.00 seconds\n",
      "Processing interval 22/29, duration: 3.00 seconds\n",
      "Processing interval 23/29, duration: 3.00 seconds\n",
      "Processing interval 24/29, duration: 3.00 seconds\n",
      "Processing interval 25/29, duration: 3.00 seconds\n",
      "Processing interval 26/29, duration: 3.00 seconds\n",
      "Processing interval 27/29, duration: 3.00 seconds\n",
      "Processing interval 28/29, duration: 3.00 seconds\n",
      "Processing interval 29/29, duration: 3.00 seconds\n",
      "f7_script3_ipad_livingroom1.wav\n",
      "Original audio duration: 183.97854875283446 seconds\n",
      "Audio duration after trimming silence: 176.92 seconds\n",
      "Audio duration after reducing noise: 99.66 seconds\n",
      "Processing interval 1/33, duration: 3.00 seconds\n",
      "Processing interval 2/33, duration: 3.00 seconds\n",
      "Processing interval 3/33, duration: 3.00 seconds\n",
      "Processing interval 4/33, duration: 3.00 seconds\n",
      "Processing interval 5/33, duration: 3.00 seconds\n",
      "Processing interval 6/33, duration: 3.00 seconds\n",
      "Processing interval 7/33, duration: 3.00 seconds\n",
      "Processing interval 8/33, duration: 3.00 seconds\n",
      "Processing interval 9/33, duration: 3.00 seconds\n",
      "Processing interval 10/33, duration: 3.00 seconds\n",
      "Processing interval 11/33, duration: 3.00 seconds\n",
      "Processing interval 12/33, duration: 3.00 seconds\n",
      "Processing interval 13/33, duration: 3.00 seconds\n",
      "Processing interval 14/33, duration: 3.00 seconds\n",
      "Processing interval 15/33, duration: 3.00 seconds\n",
      "Processing interval 16/33, duration: 3.00 seconds\n",
      "Processing interval 17/33, duration: 3.00 seconds\n",
      "Processing interval 18/33, duration: 3.00 seconds\n",
      "Processing interval 19/33, duration: 3.00 seconds\n",
      "Processing interval 20/33, duration: 3.00 seconds\n",
      "Processing interval 21/33, duration: 3.00 seconds\n",
      "Processing interval 22/33, duration: 3.00 seconds\n",
      "Processing interval 23/33, duration: 3.00 seconds\n",
      "Processing interval 24/33, duration: 3.00 seconds\n",
      "Processing interval 25/33, duration: 3.00 seconds\n",
      "Processing interval 26/33, duration: 3.00 seconds\n",
      "Processing interval 27/33, duration: 3.00 seconds\n",
      "Processing interval 28/33, duration: 3.00 seconds\n",
      "Processing interval 29/33, duration: 3.00 seconds\n",
      "Processing interval 30/33, duration: 3.00 seconds\n",
      "Processing interval 31/33, duration: 3.00 seconds\n",
      "Processing interval 32/33, duration: 3.00 seconds\n",
      "Processing interval 33/33, duration: 3.00 seconds\n",
      "f1_script2_ipad_livingroom1.wav\n",
      "Original audio duration: 178.52748299319728 seconds\n",
      "Audio duration after trimming silence: 170.32 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^(f1_|f7|f8|m3|m6|m8)\u001b[39m\u001b[38;5;124m'\u001b[39m, filename)) \u001b[38;5;129;01mand\u001b[39;00m (re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.wav$\u001b[39m\u001b[38;5;124m'\u001b[39m, filename)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mstartswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m---> 11\u001b[0m         \u001b[43mplot_spectrogram_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(filename)\n",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m, in \u001b[0;36mplot_spectrogram_main\u001b[0;34m(file_path, file_name, class_val)\u001b[0m\n\u001b[1;32m     47\u001b[0m     processed_segments\u001b[38;5;241m.\u001b[39mappend(segment)\n\u001b[1;32m     48\u001b[0m signal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(processed_segments)  \u001b[38;5;66;03m# Combine segments\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m reduced_noise \u001b[38;5;241m=\u001b[39m \u001b[43mnr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignal_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m signal \u001b[38;5;241m=\u001b[39m reduced_noise\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio duration after reducing noise: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(signal)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39msignal_rate\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/MLLab/.venv/lib/python3.12/site-packages/noisereduce/noisereduce.py:185\u001b[0m, in \u001b[0;36mreduce_noise\u001b[0;34m(y, sr, stationary, y_noise, prop_decrease, time_constant_s, freq_mask_smooth_hz, time_mask_smooth_ms, thresh_n_mult_nonstationary, sigmoid_slope_nonstationary, n_std_thresh_stationary, tmp_folder, chunk_size, padding, n_fft, win_length, hop_length, clip_noise_stationary, use_tqdm, n_jobs, use_torch, device)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m         sg \u001b[38;5;241m=\u001b[39m SpectralGateNonStationary(\n\u001b[1;32m    168\u001b[0m             y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    169\u001b[0m             sr\u001b[38;5;241m=\u001b[39msr,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    183\u001b[0m             n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    184\u001b[0m         )\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_traces\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/MLLab/.venv/lib/python3.12/site-packages/noisereduce/spectralgate/base.py:206\u001b[0m, in \u001b[0;36mSpectralGate.get_traces\u001b[0;34m(self, start_frame, end_frame)\u001b[0m\n\u001b[1;32m    203\u001b[0m     end_list\u001b[38;5;241m.\u001b[39mappend(end0)\n\u001b[1;32m    204\u001b[0m     pos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m end0 \u001b[38;5;241m-\u001b[39m start0\n\u001b[0;32m--> 206\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterate_chunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiltered_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mich\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mich\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mich1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mich2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m filtered_chunk\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype)\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[0;32m~/Developer/MLLab/.venv/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Developer/MLLab/.venv/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Developer/MLLab/.venv/lib/python3.12/site-packages/noisereduce/spectralgate/base.py:163\u001b[0m, in \u001b[0;36mSpectralGate._iterate_chunk\u001b[0;34m(self, filtered_chunk, pos, end0, start0, ich)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_iterate_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, filtered_chunk, pos, end0, start0, ich):\n\u001b[0;32m--> 163\u001b[0m     filtered_chunk0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_filtered_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mich\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     filtered_chunk[:, pos: pos \u001b[38;5;241m+\u001b[39m end0 \u001b[38;5;241m-\u001b[39m start0] \u001b[38;5;241m=\u001b[39m filtered_chunk0[:, start0:end0]\n\u001b[1;32m    165\u001b[0m     pos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m end0 \u001b[38;5;241m-\u001b[39m start0\n",
      "File \u001b[0;32m~/Developer/MLLab/.venv/lib/python3.12/site-packages/noisereduce/spectralgate/base.py:156\u001b[0m, in \u001b[0;36mSpectralGate._get_filtered_chunk\u001b[0;34m(self, ind)\u001b[0m\n\u001b[1;32m    154\u001b[0m start0 \u001b[38;5;241m=\u001b[39m ind \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk_size\n\u001b[1;32m    155\u001b[0m end0 \u001b[38;5;241m=\u001b[39m (ind \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chunk_size\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend0\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/MLLab/.venv/lib/python3.12/site-packages/noisereduce/spectralgate/base.py:149\u001b[0m, in \u001b[0;36mSpectralGate.filter_chunk\u001b[0;34m(self, start_frame, end_frame)\u001b[0m\n\u001b[1;32m    147\u001b[0m i2 \u001b[38;5;241m=\u001b[39m end_frame \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding\n\u001b[1;32m    148\u001b[0m padded_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_chunk(i1, i2)\n\u001b[0;32m--> 149\u001b[0m filtered_padded_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_chunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m filtered_padded_chunk[:, start_frame \u001b[38;5;241m-\u001b[39m i1: end_frame \u001b[38;5;241m-\u001b[39m i1]\n",
      "File \u001b[0;32m~/Developer/MLLab/.venv/lib/python3.12/site-packages/noisereduce/spectralgate/nonstationary.py:101\u001b[0m, in \u001b[0;36mSpectralGateNonStationary._do_filter\u001b[0;34m(self, chunk)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_do_filter\u001b[39m(\u001b[38;5;28mself\u001b[39m, chunk):\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Do the actual filtering\"\"\"\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     chunk_filtered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectral_gating_nonstationary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunk_filtered\n",
      "File \u001b[0;32m~/Developer/MLLab/.venv/lib/python3.12/site-packages/noisereduce/spectralgate/nonstationary.py:59\u001b[0m, in \u001b[0;36mSpectralGateNonStationary.spectral_gating_nonstationary\u001b[0;34m(self, chunk)\u001b[0m\n\u001b[1;32m     51\u001b[0m _, _, sig_stft \u001b[38;5;241m=\u001b[39m stft(\n\u001b[1;32m     52\u001b[0m     channel,\n\u001b[1;32m     53\u001b[0m     nfft\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_fft,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m     padded\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# get abs of signal stft\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m abs_sig_stft \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig_stft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# get the smoothed mean of the signal\u001b[39;00m\n\u001b[1;32m     62\u001b[0m sig_stft_smooth \u001b[38;5;241m=\u001b[39m get_time_smoothed_representation(\n\u001b[1;32m     63\u001b[0m     abs_sig_stft,\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msr,\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hop_length,\n\u001b[1;32m     66\u001b[0m     time_constant_s\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time_constant_s,\n\u001b[1;32m     67\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# assign directory\n",
    "directory = \"AudioFiles\"\n",
    "x = 0\n",
    "\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in files:\n",
    "        if (re.match(r'^(f1_|f7|f8|m3|m6|m8)', filename)) and (re.match(r'.*\\.wav$', filename)) and not filename.startswith((\".\", \"_\")):\n",
    "            plot_spectrogram_main(root, filename, 1)\n",
    "            x += 1\n",
    "            print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating spectrograms for rejecting class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/serhatseval/Developer/MLLab/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "main() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^(f1_|f7|f8|m3|m6|m8)\u001b[39m\u001b[38;5;124m'\u001b[39m, filename)) \u001b[38;5;129;01mand\u001b[39;00m (re\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.wav$\u001b[39m\u001b[38;5;124m'\u001b[39m, filename)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mstartswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m---> 13\u001b[0m         \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m         x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(filename)\n",
      "\u001b[0;31mTypeError\u001b[0m: main() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# import required module\n",
    "import os\n",
    "import re\n",
    "from Plot_Spectrograms import main\n",
    "\n",
    "# assign directory\n",
    "directory = \"AudioFiles\"\n",
    "x = 0\n",
    "\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for filename in files:\n",
    "        if (not re.match(r'^(f1_|f7|f8|m3|m6|m8)', filename)) and (re.match(r'.*\\.wav$', filename)) and not filename.startswith((\".\", \"_\")):\n",
    "            main(root, filename, 0)\n",
    "            x += 1\n",
    "            print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Model we will use the labels that we removed labels with 95% similarity with Pruning. We have prepared those labels and it is provided with the spectrograms. Training is done following way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import datetime\n",
    "\n",
    "# Paths\n",
    "annotations_file = 'MEL_Spectograms/labels_removing_95_percent_similarity.csv'\n",
    "img_dir = 'MEL_Spectograms/images'\n",
    "\n",
    "# Constants\n",
    "img_height = 1000\n",
    "img_width = 400  # Length for 3 seconds of audio\n",
    "features = 2\n",
    "batch_size = 16\n",
    "epochs = 10\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path, mode=ImageReadMode.GRAY)\n",
    "        label = int(self.img_labels.iloc[idx, 1])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "\n",
    "# Device configuration\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# Model creation\n",
    "def create_model():\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "    # Add dropout layer before the final fully connected layer\n",
    "    num_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(p=0.5),  # 50% dropout rate\n",
    "        nn.Linear(num_features, features)\n",
    "    )\n",
    "    return model.to(device)\n",
    "\n",
    "# Prune the model\n",
    "def prune_model(model, amount=0.5):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "            prune.remove(module, 'weight')\n",
    "    return model\n",
    "\n",
    "# Training loop\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "# Validation loop\n",
    "def validate(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    val_loss, correct = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            val_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            all_preds.extend(pred.argmax(1).cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return val_loss, correct, f1, cm\n",
    "\n",
    "# Save results\n",
    "def save_results(filename, accuracy, avg_loss, f1_score, cm):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(f\"Accuracy: {accuracy:.4f}, Avg Loss: {avg_loss:.4f}, F1 Score: {f1_score:.4f}\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(f\"{cm}\\n\")\n",
    "\n",
    "# Calculate class weights\n",
    "def calculate_class_weights(dataset):\n",
    "    labels = [dataset[i][1] for i in range(len(dataset))]\n",
    "    class_sample_counts = pd.Series(labels).value_counts().sort_index()\n",
    "    weights = 1.0 / class_sample_counts\n",
    "    class_weights = torch.tensor(weights / weights.sum(), dtype=torch.float32).to(device)\n",
    "    return class_weights\n",
    "\n",
    "# Group intervals by audio file\n",
    "def get_audio_file_groups(annotations_file):\n",
    "    df = pd.read_csv(annotations_file)\n",
    "    groups = defaultdict(list)\n",
    "    for idx, row in df.iterrows():\n",
    "        audio_file = row[0].split('_i')[0]  # Extract the audio file part before '_i'\n",
    "        groups[audio_file].append(idx)  # Map audio file to its indices\n",
    "    return groups\n",
    "\n",
    "# Split audio files into train and test sets\n",
    "def split_by_audio_file(groups, test_size=0.2, random_state=42):\n",
    "    audio_files = list(groups.keys())\n",
    "    train_files, test_files = train_test_split(audio_files, test_size=test_size, random_state=random_state)\n",
    "    train_indices = [idx for file in train_files for idx in groups[file]]\n",
    "    test_indices = [idx for file in test_files for idx in groups[file]]\n",
    "    return train_indices, test_indices\n",
    "\n",
    "# Main logic\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Defining Dataset\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ConvertImageDtype(torch.float32),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    dataset = CustomImageDataset(annotations_file, img_dir, transform=transform)\n",
    "    groups = get_audio_file_groups(annotations_file)\n",
    "    train_idx, test_idx = split_by_audio_file(groups, test_size=0.2)\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_idx)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    class_weights = calculate_class_weights(train_dataset)\n",
    "    print(f\"Class Weights: {class_weights}\")\n",
    "\n",
    "    model = create_model()\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "        val_loss, accuracy, f1_score, cm = validate(test_dataloader, model, loss_fn)\n",
    "        scheduler.step()\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "        # Save the model after each epoch\n",
    "        torch.save(model.state_dict(), f\"Models/FinalPruning/model_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "    train_loss, train_accuracy, train_f1, train_cm = validate(train_dataloader, model, loss_fn)\n",
    "    test_loss, test_accuracy, test_f1, test_cm = validate(test_dataloader, model, loss_fn)\n",
    "\n",
    "    save_results(\"Train_Results_under95_final.txt\", train_accuracy, train_loss, train_f1, train_cm)\n",
    "    save_results(\"Test_Results_under95_final.txt\", test_accuracy, test_loss, test_f1, test_cm)\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "    # Prune the model\n",
    "    pruned_model = prune_model(model, amount=0.5)\n",
    "\n",
    "    # Retrain the pruned model\n",
    "    optimizer = torch.optim.Adam(pruned_model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Retraining Epoch {epoch + 1}\\n-------------------------------\")\n",
    "        train(train_dataloader, pruned_model, loss_fn, optimizer)\n",
    "        val_loss, accuracy, f1_score, cm = validate(test_dataloader, pruned_model, loss_fn)\n",
    "        scheduler.step()\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1_score:.4f}\")\n",
    "\n",
    "        # Save the pruned model after each epoch\n",
    "        torch.save(pruned_model.state_dict(), f\"Models/Pruned/model_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "    train_loss, train_accuracy, train_f1, train_cm = validate(train_dataloader, pruned_model, loss_fn)\n",
    "    test_loss, test_accuracy, test_f1, test_cm = validate(test_dataloader, pruned_model, loss_fn)\n",
    "\n",
    "    save_results(\"Train_Results_pruned_final.txt\", train_accuracy, train_loss, train_f1, train_cm)\n",
    "    save_results(\"Test_Results_pruned_final.txt\", test_accuracy, test_loss, test_f1, test_cm)\n",
    "\n",
    "    print(\"Pruning and retraining done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
